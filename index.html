<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Jingfeng Wu</title>

  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="dark light">
  <meta name="theme-color" content="#FFFFFF" media="(prefers-color-scheme: light)">
  <meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="myweb.css">

  <link rel="icon" type="image/svg+xml" href="icons/favicon.svg">
  <link rel="icon" type="image/png" href="icons/favicon.png">
</head>

<body>
  <div class="section">
    <div id="bio">
      <div class="biotext">
        <h1>
          Jingfeng Wu &nbsp;|&nbsp; <span style="white-space: nowrap" lang="zh">吴京风</span>
        </h1>
        <p>
          I am a postdoc fellow at the <a href="https://simons.berkeley.edu/homepage">Simons Institute</a>
          at UC Berkeley hosted by <a href="https://www.stat.berkeley.edu/~bartlett">Peter Bartlett</a> and <a
            href="https://binyu.stat.berkeley.edu">Bin Yu</a>.
          I am a part of the NSF/Simons <a href="https://deepfoundations.ai">Collaboration on the Theoretical
            Foundations
            of Deep Learning</a>.
          I received my Ph.D. in Computer Science at Johns Hopkins University, advised by <a
            href="https://www.cs.jhu.edu/~vova">Vladimir Braverman</a>.
        </p>

        <p>
          Contact me via <a href="mailto:uuujf@berkeley.edu">Email</a>.
        </p>

        <details>
          <summary><strong>Short bio</strong></summary>
          <p>
            Jingfeng Wu is a postdoctoral fellow at the Simons Institute for the Theory of Computing at UC Berkeley. 
            His research focuses on deep learning theory, optimization, and statistical learning. 
            He earned his Ph.D. in Computer Science from Johns Hopkins University in 2023. 
            Prior to that, he received a B.S. in Mathematics (2016) and an M.S. in Applied Mathematics (2019), both from Peking University. 
            In 2023, he was recognized as a Rising Star in Data Science by the University of Chicago and UC San Diego.
          </p>
        </details>


        <!-- <p style="text-align:center">
          <a href="mailto:uuujf@berkeley.edu">Email</a> &nbsp;|&nbsp;
          <a href="https://arxiv.org/a/wu_j_11.html">arXiv</a> &nbsp;|&nbsp;
          <a
            href="https://scholar.google.com/citations?hl=en&user=z-KILD8AAAAJ&view_op=list_works&sortby=pubdate">Google&nbsp;Scholar</a>
          &nbsp;|&nbsp; <a rel="me" href="https://sigmoid.social/@uuujf">Mastodon</a> 
        </p> -->
      </div>
      <img src="icons/jwu_medium.png" alt="JW" class="bioimage">
    </div>
  </div>

  <div class="section">
    <h2>Research</h2>
    <div class="content">
      <!-- <p>
        My research focuses on bridging the gap between theory and practice in machine learning by developing efficient
        algorithms to solve real-world problems and providing a deep understanding of the underlying theoretical
        principles.
      </p> -->
      <p>I work on the theory and algorithms for machine learning. I am interested in 
        <ul>
          <li>deep learning theory</li>
          <li>optimization</li>
          <li>statistical learning</li>
        </ul>
      </p>
      <p>
        Find my papers on <a href="https://arxiv.org/a/wu_j_11.html">arXiv</a> and <a href="https://scholar.google.com/citations?hl=en&user=z-KILD8AAAAJ&view_op=list_works&sortby=pubdate">Google&nbsp;Scholar</a>.
      </p>
    </div>
  </div>

  <div class="section">
    <h2>News</h2>
    <div class="content">
      <ul>
        <li>[07/2025] Invited talk at the 6th Youth in High-Dimensions Conference.</li>
        <li>[05/2025] Invited talk at SIAM DS25.</li>
        <li>[05/2025] Three papers accepted to ICML 2025.</li>
        <li>[02/2025] Organizing a <a href="https://simons.berkeley.edu/workshops/deep-learning-theory">deep learning
            theory workshop</a> at Simons.</li>
        <li>[01/2025] One paper accepted to ICLR 2025.</li>
      </ul>
      <details>
        <summary style="padding: 0px 0px 0px 30px;"><strong>Prior to 2025</strong></summary>
        <ul>
          <li>[09/2024] Three papers accepted to NeurIPS 2024; huge congrats to Yuhang, Licong, and Ruiqi!!</li>
          <li>[05/2024] One paper accepted to COLT 2024.</li>
          <li>[03/2024] Invited talk at UCLA CS.</li>
          <li>[02/2024] Invited talk at UC Berkeley Biostatistics.</li>
          <li>[01/2024] Two papers accepted to ICLR 2024.</li>
          <li>[10/2023] Selected as Rising Star in Data Science by UChicago and UCSD.</li>
          <li>[09/2023] Two papers accepted to NeurIPS 2023.</li>
          <li>[08/2023] Joining the Simons Institute at UC Berkeley as a postdoc.</li>
          <li>[06/2023] Defended my PhD dissertation!</li>
          <li>[05/2023] One paper accepted to CoLLAs 2023; congrats to Haoran!</li>
          <li>[04/2023] One paper accepted to ICML 2023.</li>
          <li>[09/2022] Two papers accepted to NeurIPS 2022.</li>
          <li>[06/2022] Interning at Google Research Seattle.</li>
          <li>[05/2022] One paper accepted to ICML 2022 as long presentation!</li>
          <li>[01/2022] One paper accepted to AISTATS 2022.</li>
          <li>[12/2021] Passed the PhD candidacy exam.</li>
          <li>[09/2021] Two papers accepted to NeurIPS 2021.</li>
          <li>[05/2021] Awarded MINDS Summer Data Science Fellowship!</li>
          <li>[05/2021] One paper accepted to COLT 2021.</li>
          <li>[02/2021] In a relationship with Yuan, happy Valentine's day~</li>
          <li>[01/2021] One paper is accepted to ICLR 2021.</li>
          <li>[05/2020] Two papers accepted to ICML 2020.</li>
          <li>[09/2019] Joining Hopkins. Veritas vos liberabit!</li>
          <li>[06/2019] Graduated from Peking University.</li>
          <li>[04/2019] One paper accepted to ICML 2019.</li>
          <li>[03/2019] One paper accepted to CVPR 2019 as an oral presentation.</li>
          <li>[12/2018] Looking for a Ph.D. position in machine learning.</li>
        </ul>
      </details>
    </div>
  </div>

  <div class="section">
    <h2>Selected Papers (<a href="papers.html">All Papers</a>)</h2>

    <span>* indicates equal contribution or alphabetical order</span>

    <div class="content">

      <h3>Optimization with Large Stepsizes (<a href="postdoc/wu2025large_slides.pdf">Slides</a>)</h3>
      <div class="content">
        <ul>

          <li>
            <p> <a href="https://arxiv.org/abs/2506.02336"><span class="paper">Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression</span></a>
              <br>
              <span class="author"><span class="me">JW*</span>, Pierre Marion*, Peter Bartlett</span><br>
              <span class="venue">arXiv 2025</span> 
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2504.04105"><span class="paper">Minimax Optimal Convergence of Gradient
                  Descent in Logistic Regression via Large and Adaptive Stepsizes</span></a>
              <br>
              <span class="author">Ruiqi Zhang, <span class="me">JW</span>, Licong Lin, Peter Bartlett</span><br>
              <span class="venue">ICML 2025</span> &nbsp;|&nbsp; <a href="postdoc/zhang2025minimax_poster.pdf">poster</a>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2402.15926"><span class="paper">Large Stepsize Gradient Descent for
                  Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
                Telgarsky*, Bin Yu*</span><br>
              <span class="venue">COLT 2024</span> &nbsp;|&nbsp;
              <a href="postdoc/wu2024large_poster.pdf">poster</a>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2305.11788"><span class="paper">Implicit Bias of Gradient Descent for
                  Logistic Regression at the Edge of Stability</span> </a>
              <br>
              <span class="author"><span class="me">JW</span>, Vladimir Braverman, Jason Lee</span><br>
              <span class="venue">NeurIPS 2023 (<strong>spotlight</strong>)</span> &nbsp;|&nbsp;
              <a href="phd/wu2023implicit_poster.pdf">poster</a>
            </p>
          </li>


        </ul>
      </div>



      <h3>Statistical Approaches to Implicit Regularization (<a href="postdoc/wu2025benefits_slides.pdf">Slides 1</a>)
      </h3>
      <div class="content">
        <ul>

          <li>
            <p> <a href="https://arxiv.org/abs/2502.13283"><span class="paper">Benefits of Early Stopping in Gradient
                  Descent for Overparameterized Logistic Regression</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
                Telgarsky*, Bin Yu*</span><br>
              <span class="venue">ICML 2025</span> &nbsp;|&nbsp;
              <a href="postdoc/wu2025benefits_poster.pdf">poster</a>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2110.06198"><span class="paper">Last Iterate Risk Bounds of SGD with
                  Decaying Stepsize for Overparameterized Linear Regression</span></a>
              <br>
              <span class="author">
                <span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">ICML 2022 (<strong>long presentation</strong>)</span> &nbsp;|&nbsp;
              <a href="phd/wu2022last_poster.pdf">poster</a>
            </p>
          </li>


          <li>
            <p>
              <a href="https://arxiv.org/abs/2103.12692">
                <span class="paper">Benign Overfitting of Constant-Stepsize SGD for Linear Regression</span>
              </a>
              <br>
              <span class="author">
                Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">COLT 2021 (<strong>journal version in JMLR 2023</strong>)</span>
            </p>
          </li>
        </ul>
      </div>




      <h3>Applications of Implicit Regularization</h3>
      <div class="content">
        <ul>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2406.08466"><span class="paper">Scaling Laws in Linear Regression:
                  Compute, Parameters, and Data</span></a>
              <br>
              <span class="author">Licong Lin, <span class="me">JW</span>, Sham Kakade, Peter Bartlett, Jason
                Lee</span><br>
              <span class="venue">NeurIPS 2024</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2310.08391"><span class="paper">How Many Pretraining Tasks Are Needed
                  for
                  In-Context Learning of Linear Regression?</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan
                Gu,
                Peter Bartlett</span><br>
              <span class="venue">ICLR 2024 (<strong>spotlight</strong>)</span> &nbsp;|&nbsp;
              <a href="postdoc/wu2023pretraining_poster.pdf">poster</a>
            </p>
          </li>


          <li>
            <p>
              <a href="https://arxiv.org/abs/2208.01857">
                <span class="paper">The Power and Limitation of Pretraining-Finetuning for Linear Regression under
                  Covariate Shift</span>
              </a>
              <br>
              <span class="author"><span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham
                Kakade</span>
              <br>
              <span class="venue"> NeurIPS 2022</span> &nbsp;|&nbsp;
              <a href="phd/wu2022power_poster.pdf">poster</a>
            </p>
          </li>

        </ul>
      </div>




      <h3>Margin Theory for Neural Networks</h3>
      <div class="content">
        <ul>

          <li>
            <p> <a href="https://arxiv.org/abs/2502.16075"><span class="paper">Implicit Bias of Gradient Descent for
                  Non-Homogeneous Deep Networks</span></a>
              <br>
              <span class="author">Yuhang Cai*, Kangjie Zhou*, <span class="me">JW</span>, Song Mei, Michael Lindsey,
                Peter Bartlett</span><br>
              <span class="venue">ICML 2025</span>
            </p>
          </li>


          <li>
            <p>
              <a href="https://arxiv.org/abs/2406.08654"><span class="paper">Large Stepsize Gradient Descent for
                  Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization</span></a>
              <br>
              <span class="author">Yuhang Cai, <span class="me">JW</span>, Song Mei, Michael Lindsey, Peter
                Bartlett</span><br>
              <span class="venue">NeurIPS 2024</span>
            </p>
          </li>




        </ul>
      </div>


    </div>
  </div>


  <div class="section">
    <h2>Invited Talks</h2>
    <div class="content">


      <ul>
        <li>
          <a href="postdoc/wu2025large_slides.pdf">
            <span class="paper">Reimagining Gradient Descent: Large Stepsize, Oscillation, and Acceleration</span>
          </a>
          <ul>
            <li>[06/2025] <span class="location">MPI & UCLA</span>, <span class="seminar">Math Machine Learning Seminar</span>, hosted by <span class="author">Guido Montufar</span></li>
            <li>[05/2025] <span class="location">SIAM DS25</span>, <span class="seminar">Dynamical Systems for Machine
                Learning</span>, hosted
              by <span class="author">Molei Tao</span></li>
            <li>[01/2025] <span class="location">UCLA</span>, <span class="seminar">Level Set Meeting</span>, hosted
              by <span class="author">Shu Liu, Stanley Osher</span></li>
            <li>[09/2024] <span class="location">Simons Foundation</span>, <span class="seminar">MoDL Annual
                Meeting</span>, hosted
              by <span class="author">Peter Bartlett, Rene Vidal</span></li>
            <li>[05/2024] <span class="location">UC San Diego</span>, <span class="seminar">MoDL Collaboration
                Meeting</span>, hosted
              by <span class="author">Chaoyue Liu et al.</span></li>
            <li>[03/2024] <span class="location">UCLA</span>, <span class="seminar">Computer Science Seminar</span>,
              hosted by <span class="author">Quanquan Gu</span></li>
            <li>[02/2024] <span class="location">UC Berkeley</span>, <span class="seminar">Biostatistics Seminar</span>,
              hosted by <span class="author">Lexin Li</span></li>
            <li>[02/2024] <span class="location">UC San Diego</span>, <span class="seminar">Group Seminar</span>, hosted by <span class="author">Mikhail Belkin</span></li>
          </ul>
        </li>
        <li>
          <a href="postdoc/wu2025benefits_slides.pdf">
            <span class="paper">A Statistical Viewpoint on Implicit Regularization: GD for Logistic Regression</span>
          </a>
          <ul>
            <li>[07/2025] <span class="location">ICTP</span>, <span class="seminar">6th Youth in High-Dimensions Conference</span>,
              hosted by <span class="author">Marco Mondelli et al.</span></li>
            
            <li>[02/2025] <span class="location">UC Berkeley</span>, <span class="seminar">Deep Learning Theory
                Workshop</span>,
              hosted by <span class="author">Peter Bartlett et al.</span></li>
          </ul>
        </li>
</ul>
        <details>
        <summary style="padding: 0px 0px 0px 30px;"><strong>Prior to 2024</strong></summary>
        <ul>
          <li>
            <a>
              <span class="paper">New Insights about SGD: Stepsize, Risk Convergence, and Implicit Regularization</span>
            </a>
            <ul>
              <li>[10/2023] <span class="location">UC Davis</span>, <span class="seminar">Statistics Seminar</span>,
              hosted by <span class="author">Xiao Hui Tai</span></li>
          </ul>
        </li>

        <li>
          <a>
            <span class="paper">Implicit Bias of Gradient Descent for Logistic Regression at the Edge of
              Stability</span>
          </a>
          <ul>
            <li>[05/2023] <span class="location">TTIC</span>, <span class="seminar">MoDL Collaboration Meeting</span>,
              hosted by <span class="author">Sam Buchanan et al.</span></li>
          </ul>
        </li>

        <li>
          <a>
            <span class="paper">The Implicit Regularization of SGD in Least Squares and Beyond</span>
          </a>
          <ul>
            <li>[01/2023] <span class="location">Rice University</span>, <span class="seminar">Algorithms and ML
                Seminar</span>, hosted by <span class="author">Anastasios Kyrillidis</span></li>
            <li>[12/2022] <span class="location">Princeton University</span>, <span class="seminar">Group Seminar</span>, hosted by <span class="author">Jason Lee</span></li>
            <li>[11/2022] <span class="location">Georgia Tech</span>, <span class="seminar">Group Seminar</span>, hosted by <span class="author">Molei Tao</span></li>
            <li>[08/2022] <span class="location">Google Research</span>, <span class="seminar">Learning Theory Seminar</span>, hosted by <span class="author">Mehryar Mohri</span></li>
            <li>[06/2022] <span class="location">MPI & UCLA</span>, <span class="seminar">Math Machine Learning Seminar</span>, hosted by <span class="author">Guido Montufar</span></li>
          </ul>
        </li>
      </ul>
</details>


    </div>
  </div>

  <div class="section">
    <h2>Services</h2>
    <div class="content">

      <ul>
        <li>
          <p><strong>Organizer</strong></p>
          <ul>
            <li>[02/2025]
              <a href="https://simons.berkeley.edu/workshops/deep-learning-theory">Deep learning theory workshop</a>,
              Simons Institute, UC Berkeley
            </li>
          </ul>
        </li>

        <li>
          <p><strong>Conference Reviewer</strong><br>
            <a href="https://icml.cc">ICML</a> (2020 - 2025),
            <a href="https://neurips.cc">NeurIPS</a> (2020 - 2025),
            <a href="https://iclr.cc">ICLR</a> (2021 - 2025),
            <a href="https://www.siam.org/conferences-events/siam-conferences/soda26">SODA</a> (2026, subreviewer),
            <a href="https://aistats.org">AISTATS</a> (2021 - 2023),
            <a href="https://www.auai.org">UAI</a> (2023),
            <a href="https://aaai.org">AAAI</a> (2021 - 2023, PC member reviewer)
          </p>
        </li>

        <li>
          <p><strong>Journal Reviewer</strong><br>
            <a href="https://www.jmlr.org">JMLR</a>,
            <a href="https://www.computer.org/csdl/journal/tp">TPAMI</a>,
            <a href="https://jmlr.org/tmlr">TMLR</a>,
            <a
              href="https://siam.org/publications/journals/siam-journal-on-mathematics-of-data-science-simods">SIMODS</a>,
            <a href="https://www.jair.org">JAIR</a>,
            <a href="https://www.appliedprobability.org">Applied Probability Journals</a>,
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18">IEEE Transactions on Information
              Theory</a>,
          </p>
        </li>

      </ul>

    </div>
  </div>

  <div class="section">
    <h2>People</h2>
    <div class="content">

      <div>
        <a href="family.html">Family</a>
      </div>

      <h3>Recent Student Collaborators</h3>
      <span class="people"><a href="https://willcai7.github.io">Yuhang Cai</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://licong-lin.github.io">Licong Lin</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://rqzhangberkeley.github.io">Ruiqi Zhang</a>
      </span> &nbsp;&nbsp;

      <h3>Collaborators</h3>
      <span class="people"><a href="https://www.stat.berkeley.edu/~bartlett">Peter Bartlett</a> </span> &nbsp;&nbsp;
      <span class="people"><a href="https://www.cs.jhu.edu/~vova">Vladimir Braverman</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://sites.google.com/view/zxchen">Zixiang Chen</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://deanfoster.net/index.pl">Dean Foster</a>
      </span> &nbsp;&nbsp;
       <span class="people"><a href="https://www.udayaghai.com">Udaya Ghai</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://web.cs.ucla.edu/~qgu">Quanquan Gu</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://kairouzp.github.io">Peter Kairouz</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://shamulent.github.io">Sham Kakade</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://jasondlee88.github.io">Jason Lee</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://scholar.google.com/citations?user=UmZnnF4AAAAJ">Haoran Li</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://scholar.google.com/citations?user=BikRqnIAAAAJ&hl=en">Xuheng Li</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://quantumtative.github.io">Michael Lindsey</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://pierremarion23.github.io">Pierre Marion</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://www.stat.berkeley.edu/~songmei/">Song Mei</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://depenm.github.io">Depen Morwani</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://cims.nyu.edu/~matus/">Matus Telgarsky</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://nikhilvyas.github.io">Nikhil Vyas</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://lyang36.github.io">Lin Yang</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://binyu.stat.berkeley.edu">Bin Yu</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://hanlin-zhang.com">Hanlin Zhang</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://wennanzhu.github.io">Wennan Zhu</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://sites.google.com/view/drzhou">Dongruo Zhou</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://kangjie287.github.io">Kangjie Zhou</a>
      </span> &nbsp;&nbsp;
      <span class="people"><a href="https://difanzou.github.io">Difan Zou</a>
      </span> &nbsp;&nbsp;
    </div>

  </div>






  <footer style="text-align:right;font-size:14px;margin-top:100px;">
    <p>
      <a href="https://github.com/jonbarron/website">credit</a> &nbsp;|&nbsp;
      <a href="https://github.com/uuujf/uuujf.github.io">source</a>
    </p>
  </footer>

</body>

</html>