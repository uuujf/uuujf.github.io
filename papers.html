<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Jingfeng's Papers</title>

  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="dark light">
  <meta name="theme-color" content="#FFFFFF" media="(prefers-color-scheme: light)">
  <meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="myweb.css">

  <link rel="icon" type="image/svg+xml" href="icons/favicon.svg">
  <link rel="icon" type="image/png" href="icons/favicon.png">
</head>

<body>

  <main>
    <section id="publication">


      <h2>Papers by <a href="index.html">Jingfeng Wu</a></h2>

      <span>* indicates equal contribution or alphabetical order</span>
      <div class="content">
        <ul>

          <li>
            <p> <a href="https://arxiv.org/abs/2506.02336"><span class="paper">Large Stepsizes Accelerate Gradient
                  Descent for Regularized Logistic Regression</span></a>
              <br>
              <span class="author"><span class="me">JW*</span>, Pierre Marion*, Peter Bartlett</span><br>
              <span class="venue">NeurIPS 2025</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2506.08415"><span class="paper">Improved Scaling Laws in Linear
                  Regression via Data Reuse</span></a>
              <br>
              <span class="author">Licong Lin, <span class="me">JW</span>, Peter Bartlett</span><br>
              <span class="venue">NeurIPS 2025</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2504.04105"><span class="paper">Minimax Optimal Convergence of Gradient
                  Descent in Logistic Regression via Large and Adaptive Stepsizes</span></a>
              <br>
              <span class="author">Ruiqi Zhang, <span class="me">JW</span>, Licong Lin, Peter Bartlett</span><br>
              <span class="venue">ICML 2025</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2502.13283"><span class="paper">Benefits of Early Stopping in
                  Gradient Descent for Overparameterized Logistic Regression</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
                Telgarsky*, Bin Yu*</span><br>
              <span class="venue">ICML 2025</span> 
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2502.16075"><span class="paper">Implicit Bias of Gradient Descent for
                  Non-Homogeneous Deep Networks</span></a>
              <br>
              <span class="author">Yuhang Cai*, Kangjie Zhou*, <span class="me">JW</span>, Song Mei, Michael
                Lindsey, Peter Bartlett</span><br>
              <span class="venue">ICML 2025</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2410.21676"><span class="paper">How Does Critical Batch Size Scale in
                  Pre-training?</span></a>
              <br>
              <span class="author">Hanlin Zhang, Depen Morwani, Nikhil Vyas, <span class="me">JW</span>, Difan Zou,
                Udaya Ghai, Dean Foster, Sham Kakade</span><br>
              <span class="venue">ICLR 2025</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2406.08654"><span class="paper">Large Stepsize Gradient Descent for
                  Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization</span></a>
              <br>
              <span class="author">Yuhang Cai, <span class="me">JW</span>, Song Mei, Michael Lindsey, Peter
                Bartlett</span><br>
              <span class="venue">NeurIPS 2024</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2406.08466"><span class="paper">Scaling Laws in Linear Regression:
                  Compute, Parameters, and Data</span></a>
              <br>
              <span class="author">Licong Lin, <span class="me">JW</span>, Sham Kakade, Peter Bartlett, Jason
                Lee</span><br>
              <span class="venue">NeurIPS 2024</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2402.14951"><span class="paper">In-Context Learning of a Linear
                  Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</span></a>
              <br>
              <span class="author">Ruiqi Zhang, <span class="me">JW</span>, Peter Bartlett</span><br>
              <span class="venue">NeurIPS 2024</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2402.15926"><span class="paper">Large Stepsize Gradient Descent for
                  Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
                Telgarsky*, Bin Yu*</span><br>
              <span class="venue">COLT 2024</span> 
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2310.08391"><span class="paper">How Many Pretraining Tasks Are Needed
                  for
                  In-Context Learning of Linear Regression?</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan
                Gu,
                Peter Bartlett</span><br>
              <span class="venue">ICLR 2024 (<strong>spotlight</strong>)</span> 
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2311.14222"><span class="paper">Risk Bounds of Accelerated SGD for
                  Overparameterized Linear Regression</span></a>
              <br>
              <span class="author">Xuheng Li, Yihe Deng, <span class="me">JW</span>, Dongruo Zhou, Quanquan
                Gu</span><br>
              <span class="venue">ICLR 2024</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2305.11788"><span class="paper">Implicit Bias of Gradient Descent for
                  Logistic Regression at the Edge of Stability</span> </a>
              <br>
              <span class="author"><span class="me">JW</span>, Vladimir Braverman, Jason Lee</span><br>
              <span class="venue">NeurIPS 2023 (<strong>spotlight</strong>)</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2306.09396"><span class="paper">Private Federated Frequency
                  Estimation:
                  Adapting to the Hardness of the Instance</span></a>
              <br>
              <span class="author"><span class="me">JW</span>, Wennan Zhu, Peter Kairouz, Vladimir
                Braverman</span><br>
              <span class="venue">NeurIPS 2023</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2303.10263"> <span class="paper"> Fixed Design Analysis of
                  Regularization-Based Continual Learning</span> </a>
              <br>
              <span class="author">Haoran Li*, <span class="me">JW</span>*, Vladimir Braverman</span><br>
              <span class="venue">CoLLAs 2023</span>
            </p>
          </li>

          <li>
            <p> <a href="https://arxiv.org/abs/2303.02255"><span class="paper">Finite-Sample Analysis of Learning
                  High-Dimensional Single ReLU Neuron</span></a>
              <br>
              <span class="author">
                <span class="me">JW</span>*, Difan Zou*, Zixiang Chen*, Vladimir Braverman,
                Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">ICML 2023</span> 
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2208.01857">
                <span class="paper">The Power and Limitation of Pretraining-Finetuning for Linear Regression under
                  Covariate Shift</span>
              </a>
              <br>
              <span class="author"><span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham
                Kakade</span>
              <br>
              <span class="venue"> NeurIPS 2022</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2203.03159">
                <span class="paper">Risk Bounds of Multi-Pass SGD for Least Squares in the Interpolation Regime
                </span>
              </a>
              <br>
              <span class="author">
                Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">NeurIPS 2022</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2110.06198">
                <span class="paper">Last Iterate Risk Bounds of SGD with Decaying Stepsize for Overparameterized
                  Linear
                  Regression</span>
              </a>
              <br>
              <span class="author">
                <span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">ICML 2022 (<strong>long presentation</strong>)</span> 
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2108.05439">
                <span class="paper">Gap-Dependent Unsupervised Exploration for Reinforcement Learning</span>
              </a>
              <br>
              <span class="author">
                <span class="me">JW</span>, Vladimir Braverman, Lin Yang
              </span>
              <br>
              <span class="venue">AISTATS 2022</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2108.04552">
                <span class="paper">The Benefits of Implicit Regularization from SGD in Least Squares Problems
                </span>
              </a>
              <br>
              <span class="author">
                Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Dean Foster,
                Sham Kakade</span>
              <br>
              <span class="venue">NeurIPS 2021</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2011.13034">
                <span class="paper">Accommodating Picky Customers: Regret Bound and Exploration Complexity for
                  Multi-Objective Reinforcement Learning</span>
              </a>
              <br><span class="author">
                <span class="me">JW</span>, Vladimir Braverman, Lin Yang</span>
              <br>
              <span class="venue">NeurIPS 2021</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2104.08604">
                <span class="paper">Lifelong Learning with Sketched Structural Regularization</span>
              </a>
              <br>
              <span class="author">
                Haoran Li, Aditya Krishnan, <span class="me">JW</span>, Soheil Kolouri, Praveen Pilly, Vladimir
                Braverman</span>
              <br>
              <span class="venue">ACML 2021</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2103.12692">
                <span class="paper">Benign Overfitting of Constant-Stepsize SGD for Linear Regression</span>
              </a>
              <br>
              <span class="author">
                Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
              <br>
              <span class="venue">COLT 2021 (<strong>journal version in JMLR 2023</strong>)</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2011.02538">
                <span class="paper">Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with
                  Moderate Learning Rate</span>
              </a>
              <br>
              <span class="author">
                <span class="me">JW</span>, Difan Zou, Vladimir Braverman, Quanquan Gu</span>
              <br>
              <span class="venue">ICLR 2021</span> 
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/2008.06736">
                <span class="paper">Obtaining Adjustable Regularization for Free via Iterate Averaging</span>
              </a>
              <br><span class="author">
                <span class="me">JW</span>, Vladimir Braverman, Lin Yang</span>
              <br>
              <span class="venue">ICML 2020</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/1906.07405">
                <span class="paper">On the Noisy Gradient Descent that Generalizes as SGD</span>
              </a>
              <br><span class="author">
                <span class="me">JW</span>, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, Zhanxing
                Zhu</span>
              <br>
              <span class="venue">ICML 2020</span> 
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/1808.06088">
                <span class="paper">Tangent-Normal Adversarial Regularization for Semi-supervised Learning
                </span>
              </a>
              <br><span class="author">
                Bing Yu*, <span class="me">JW</span>*, Jinwen Ma, Zhanxing Zhu</span>
              <br>
              <span class="venue">CVPR 2019 (<strong>oral</strong>)</span>
            </p>
          </li>

          <li>
            <p>
              <a href="https://arxiv.org/abs/1803.00195">
                <span class="paper">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping
                  from
                  Minima and Regularization Effects</span>
              </a>
              <br>
              <span class="author">Zhanxing Zhu*, <span class="me">JW</span>*, Bing Yu, Lei Wu,
                Jinwen Ma</span>
              <br>
              <span class="venue">ICML 2019</span>
            </p>
          </li>
        </ul>
      </div>


    </section>
  </main>

</body>

</html>