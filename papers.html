<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Jingfeng Wu</title>

  <meta charset="utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="dark light">
  <meta name="theme-color" content="#FFFFFF" media="(prefers-color-scheme: light)">
  <meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="myweb.css">

  <link rel="icon" type="image/svg+xml" href="icons/favicon.svg">
  <link rel="icon" type="image/png" href="icons/favicon.png">
</head>

<body>


  <div class="section">


    <h2>Papers by <a href="index.html">Jingfeng Wu</a> (<a href="https://arxiv.org/a/wu_j_11.html">arXiv</a>, <a
        href="https://scholar.google.com/citations?hl=en&user=z-KILD8AAAAJ&view_op=list_works&sortby=pubdate">Google&nbsp;Scholar</a>)</h2>

        <span>* indicates equal contribution or alphabetical order</span>
    <div class="content">
      <ul>

        <li>
          <p> <a href="https://arxiv.org/abs/2506.08415"><span class="paper">Improved Scaling Laws in Linear Regression via Data Reuse</span></a>
              <br>
              <span class="author">Licong Lin, <span class="me">JW</span>, Peter Bartlett</span><br>
              <span class="venue">arXiv 2025</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2506.02336"><span class="paper">Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression</span></a>
              <br>
              <span class="author"><span class="me">JW*</span>, Pierre Marion*, Peter Bartlett</span><br>
              <span class="venue">arXiv 2025</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2504.04105"><span class="paper">Minimax Optimal Convergence of Gradient Descent in Logistic Regression via Large and Adaptive Stepsizes</span></a>
            <br>
            <span class="author">Ruiqi Zhang, <span class="me">JW</span>, Licong Lin, Peter Bartlett</span><br>
            <span class="venue">ICML 2025</span> &nbsp;|&nbsp; <a href="postdoc/zhang2025minimax_poster.pdf">poster</a>
          </p>
        </li>


        <li>
          <p> <a href="https://arxiv.org/abs/2502.13283"><span class="paper">Benefits of Early Stopping in
                Gradient Descent for Overparameterized Logistic Regression</span></a>
            <br>
            <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
              Telgarsky*, Bin Yu*</span><br>
            <span class="venue">ICML 2025</span> &nbsp;|&nbsp;
              <a href="postdoc/wu2025benefits_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2502.16075"><span class="paper">Implicit Bias of Gradient Descent for
                Non-Homogeneous Deep Networks</span></a>
            <br>
            <span class="author">Yuhang Cai*, Kangjie Zhou*, <span class="me">JW</span>, Song Mei, Michael
              Lindsey, Peter Bartlett</span><br>
            <span class="venue">ICML 2025</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2504.04039"><span class="paper">Memory-Statistics Tradeoff in Continual Learning with Structural Regularization</span></a>
            <br>
            <span class="author">Haoran Li, <span class="me">JW</span>, Vladimir Braverman</span><br>
            <span class="venue">arXiv 2025</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2410.21676"><span class="paper">How Does Critical Batch Size Scale in
                Pre-training?</span></a>
            <br>
            <span class="author">Hanlin Zhang, Depen Morwani, Nikhil Vyas, <span class="me">JW</span>, Difan Zou,
              Udaya Ghai, Dean Foster, Sham Kakade</span><br>
            <span class="venue">ICLR 2025</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2410.12783"><span class="paper">Context-Scaling versus Task-Scaling in
                In-Context Learning</span></a>
            <br>
            <span class="author">Amirhesam Abedsoltan, Adityanarayanan Radhakrishnan, <span class="me">JW</span>,
              Mikhail Belkin</span><br>
            <span class="venue">arXiv 2024</span>
          </p>
        </li>


        <li>
          <p>
            <a href="https://arxiv.org/abs/2406.08654"><span class="paper">Large Stepsize Gradient Descent for
                Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization</span></a>
            <br>
            <span class="author">Yuhang Cai, <span class="me">JW</span>, Song Mei, Michael Lindsey, Peter
              Bartlett</span><br>
            <span class="venue">NeurIPS 2024</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2406.08466"><span class="paper">Scaling Laws in Linear Regression:
                Compute, Parameters, and Data</span></a>
            <br>
            <span class="author">Licong Lin, <span class="me">JW</span>, Sham Kakade, Peter Bartlett, Jason
              Lee</span><br>
            <span class="venue">NeurIPS 2024</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2402.14951"><span class="paper">In-Context Learning of a Linear
                Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</span></a>
            <br>
            <span class="author">Ruiqi Zhang, <span class="me">JW</span>, Peter Bartlett</span><br>
            <span class="venue">NeurIPS 2024</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2402.15926"><span class="paper">Large Stepsize Gradient Descent for
                Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency</span></a>
            <br>
            <span class="author"><span class="me">JW</span>, Peter Bartlett*, Matus
              Telgarsky*, Bin Yu*</span><br>
            <span class="venue">COLT 2024</span> &nbsp;|&nbsp;
            <a href="postdoc/wu2024large_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2310.08391"><span class="paper">How Many Pretraining Tasks Are Needed
                for
                In-Context Learning of Linear Regression?</span></a>
            <br>
            <span class="author"><span class="me">JW</span>, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan
              Gu,
              Peter Bartlett</span><br>
            <span class="venue">ICLR 2024 (<strong>spotlight</strong>)</span> &nbsp;|&nbsp;
            <a href="postdoc/wu2023pretraining_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2311.14222"><span class="paper">Risk Bounds of Accelerated SGD for
                Overparameterized Linear Regression</span></a>
            <br>
            <span class="author">Xuheng Li, Yihe Deng, <span class="me">JW</span>, Dongruo Zhou, Quanquan
              Gu</span><br>
            <span class="venue">ICLR 2024</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2305.11788"><span class="paper">Implicit Bias of Gradient Descent for
                Logistic Regression at the Edge of Stability</span> </a>
            <br>
            <span class="author"><span class="me">JW</span>, Vladimir Braverman, Jason Lee</span><br>
            <span class="venue">NeurIPS 2023 (<strong>spotlight</strong>)</span> &nbsp;|&nbsp;
            <a href="phd/wu2023implicit_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2306.09396"><span class="paper">Private Federated Frequency
                Estimation:
                Adapting to the Hardness of the Instance</span></a>
            <br>
            <span class="author"><span class="me">JW</span>, Wennan Zhu, Peter Kairouz, Vladimir
              Braverman</span><br>
            <span class="venue">NeurIPS 2023</span> 
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2303.10263"> <span class="paper"> Fixed Design Analysis of
                Regularization-Based Continual Learning</span> </a>
            <br>
            <span class="author">Haoran Li*, <span class="me">JW</span>*, Vladimir Braverman</span><br>
            <span class="venue">CoLLAs 2023</span>
          </p>
        </li>

        <li>
          <p> <a href="https://arxiv.org/abs/2303.02255"><span class="paper">Finite-Sample Analysis of Learning
                High-Dimensional Single ReLU Neuron</span></a>
            <br>
            <span class="author">
              <span class="me">JW</span>*, Difan Zou*, Zixiang Chen*, Vladimir Braverman,
              Quanquan Gu, Sham Kakade</span>
            <br>
            <span class="venue">ICML 2023</span> &nbsp;|&nbsp;
            <a href="phd/wu2023finite_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2208.01857">
              <span class="paper">The Power and Limitation of Pretraining-Finetuning for Linear Regression under
                Covariate Shift</span>
            </a>
            <br>
            <span class="author"><span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham
              Kakade</span>
            <br>
            <span class="venue"> NeurIPS 2022</span> &nbsp;|&nbsp;
            <a href="phd/wu2022power_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2203.03159">
              <span class="paper">Risk Bounds of Multi-Pass SGD for Least Squares in the Interpolation Regime
              </span>
            </a>
            <br>
            <span class="author">
              Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
            <br>
            <span class="venue">NeurIPS 2022</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2110.06198">
              <span class="paper">Last Iterate Risk Bounds of SGD with Decaying Stepsize for Overparameterized
                Linear
                Regression</span>
            </a>
            <br>
            <span class="author">
              <span class="me">JW</span>*, Difan Zou*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
            <br>
            <span class="venue">ICML 2022 (<strong>long presentation</strong>)</span> &nbsp;|&nbsp;
            <a href="phd/wu2022last_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2108.05439">
              <span class="paper">Gap-Dependent Unsupervised Exploration for Reinforcement Learning</span>
            </a>
            <br>
            <span class="author">
              <span class="me">JW</span>, Vladimir Braverman, Lin Yang
            </span>
            <br>
            <span class="venue">AISTATS 2022</span> 
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2108.04552">
              <span class="paper">The Benefits of Implicit Regularization from SGD in Least Squares Problems
              </span>
            </a>
            <br>
            <span class="author">
              Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Dean Foster,
              Sham Kakade</span>
            <br>
            <span class="venue">NeurIPS 2021</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2011.13034">
              <span class="paper">Accommodating Picky Customers: Regret Bound and Exploration Complexity for
                Multi-Objective Reinforcement Learning</span>
            </a>
            <br><span class="author">
              <span class="me">JW</span>, Vladimir Braverman, Lin Yang</span>
            <br>
            <span class="venue">NeurIPS 2021</span> 
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2104.08604">
              <span class="paper">Lifelong Learning with Sketched Structural Regularization</span>
            </a>
            <br>
            <span class="author">
              Haoran Li, Aditya Krishnan, <span class="me">JW</span>, Soheil Kolouri, Praveen Pilly, Vladimir
              Braverman</span>
            <br>
            <span class="venue">ACML 2021</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2103.12692">
              <span class="paper">Benign Overfitting of Constant-Stepsize SGD for Linear Regression</span>
            </a>
            <br>
            <span class="author">
              Difan Zou*, <span class="me">JW</span>*, Vladimir Braverman, Quanquan Gu, Sham Kakade</span>
            <br>
            <span class="venue">COLT 2021 (<strong>journal version in JMLR 2023</strong>)</span>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2011.02538">
              <span class="paper">Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with
                Moderate Learning Rate</span>
            </a>
            <br>
            <span class="author">
              <span class="me">JW</span>, Difan Zou, Vladimir Braverman, Quanquan Gu</span>
            <br>
            <span class="venue">ICLR 2021</span> &nbsp;|&nbsp;
            <a href="phd/wu2021direction_poster.pdf">poster</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/2008.06736">
              <span class="paper">Obtaining Adjustable Regularization for Free via Iterate Averaging</span>
            </a>
            <br><span class="author">
              <span class="me">JW</span>, Vladimir Braverman, Lin Yang</span>
            <br>
            <span class="venue">ICML 2020</span> &nbsp;|&nbsp;
            <a href="https://github.com/uuujf/IterAvg">code</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/1906.07405">
              <span class="paper">On the Noisy Gradient Descent that Generalizes as SGD</span>
            </a>
            <br><span class="author">
              <span class="me">JW</span>, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, Zhanxing
              Zhu</span>
            <br>
            <span class="venue">ICML 2020</span> &nbsp;|&nbsp;
            <a href="https://github.com/uuujf/MultiNoise">code</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/1808.06088">
              <span class="paper">Tangent-Normal Adversarial Regularization for Semi-supervised Learning
              </span>
            </a>
            <br><span class="author">
              Bing Yu*, <span class="me">JW</span>*, Jinwen Ma, Zhanxing Zhu</span>
            <br>
            <span class="venue">CVPR 2019 (<strong>oral</strong>)</span> &nbsp;|&nbsp;
            <a href="https://github.com/uuujf/TNAR">code</a>
          </p>
        </li>

        <li>
          <p>
            <a href="https://arxiv.org/abs/1803.00195">
              <span class="paper">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping
                from
                Minima and Regularization Effects</span>
            </a>
            <br>
            <span class="author">Zhanxing Zhu*, <span class="me">JW</span>*, Bing Yu, Lei Wu,
              Jinwen Ma</span>
            <br>
            <span class="venue">ICML 2019</span> &nbsp;|&nbsp;
            <a href="https://github.com/uuujf/SGDNoise">code</a>
          </p>
        </li>
      </ul>
    </div>


  </div>


</body>

</html>